{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install sentence-transformers transformers\n"
      ],
      "metadata": {
        "id": "P7oPiwmWLm--"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "y1lhn69UK9aS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import RobertaModel, RobertaTokenizer, XLNetModel, XLNetTokenizer\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A journey of a thousand miles begins with a single step.\"\n",
        "]\n",
        "\n",
        "model_sentences = {\n",
        "    \"BERT\": [\n",
        "        \"The fast brown fox jumps over the lazy dog.\",\n",
        "        \"An expedition of a thousand kilometers starts with a single stride.\"\n",
        "    ],\n",
        "    \"USE\": [\n",
        "        \"A speedy brown fox leaps over the inactive canine.\",\n",
        "        \"Embarking on a lengthy trek initiates with a solitary footfall.\"\n",
        "    ],\n",
        "    \"Tf-Idf\": [\n",
        "        \"The swift brown fox leaps over the lazy dog.\",\n",
        "        \"Commencing a lengthy journey starts with one initial step.\"\n",
        "    ],\n",
        "    \"RoBERTa\": [\n",
        "        \"A nimble brown fox vaults over the indolent dog.\",\n",
        "        \"Setting off on a lengthy quest begins with a lone footstep.\"\n",
        "    ],\n",
        "    \"XLNet\": [\n",
        "        \"The agile brown fox hops over the lethargic dog.\",\n",
        "        \"Initiating a prolonged expedition starts with a single pace.\"\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "D6diURi3Mgyx"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_sentence_embedding(sentence, model):\n",
        "    if model == \"BERT\":\n",
        "        model_name = \"bert-base-uncased\"\n",
        "        embedder = SentenceTransformer(model_name)\n",
        "        return embedder.encode(sentence, convert_to_tensor=True).numpy()\n",
        "\n",
        "    elif model == \"USE\":\n",
        "        model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
        "        embedder = SentenceTransformer(model_name)\n",
        "        return embedder.encode(sentence, convert_to_tensor=True).numpy()\n",
        "    elif model == \"Tf-Idf\":\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        vectors = vectorizer.fit_transform([sentence] + model_sentences[model])\n",
        "        return vectors.toarray()[0].reshape(1, -1)\n",
        "    elif model == \"RoBERTa\":\n",
        "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "        model = RobertaModel.from_pretrained('roberta-base')\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    elif model == \"XLNet\":\n",
        "        tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "        model = XLNetModel.from_pretrained('xlnet-base-cased')\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model} not supported.\")\n"
      ],
      "metadata": {
        "id": "20ZcZCaDMlT7"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarities = {}\n",
        "max_length = 100\n",
        "\n",
        "for model, sentences in model_sentences.items():\n",
        "    model_vectors = [get_sentence_embedding(sentence, model) for sentence in sentences + reference_sentences]\n",
        "    flattened_model_vectors = [embedding.flatten()[:max_length] for embedding in model_vectors]\n",
        "    flattened_model_vectors = [np.pad(embedding, (0, max_length - len(embedding)))[:max_length] for embedding in flattened_model_vectors]\n",
        "    flattened_model_vectors_np = np.array(flattened_model_vectors)\n",
        "    cosine_similarities[model] = cosine_similarity(flattened_model_vectors_np)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObLAGEoMMnuv",
        "outputId": "2d992308-2548-426f-9067-26f8b21adad0"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity_table = pd.DataFrame({model: similarities.flatten() for model, similarities in cosine_similarities.items()})\n",
        "print(cosine_similarity_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlNFrITXXb35",
        "outputId": "c92cc56b-58e4-4b71-8657-1de62fdcf88f"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        BERT       USE    Tf-Idf   RoBERTa     XLNet\n",
            "0   1.000000  1.000000  1.000000  1.000000  1.000000\n",
            "1   0.282296  0.184619  0.000000  0.963622  0.334462\n",
            "2   0.983279  0.902487  0.454021  0.993482  0.802075\n",
            "3   0.316642  0.094034  0.553074  0.978428  0.226346\n",
            "4   0.282296  0.184619  0.000000  0.963622  0.334462\n",
            "5   1.000000  1.000000  1.000000  1.000000  1.000000\n",
            "6   0.333670  0.178470  0.231880  0.957064  0.347713\n",
            "7   0.861526  0.709835  0.136232  0.985813  0.488873\n",
            "8   0.983279  0.902487  0.454021  0.993482  0.802075\n",
            "9   0.333670  0.178470  0.231880  0.957064  0.347713\n",
            "10  1.000000  1.000000  1.000000  1.000000  1.000000\n",
            "11  0.351203  0.066233  0.493564  0.972598  0.322743\n",
            "12  0.316642  0.094034  0.553074  0.978428  0.226346\n",
            "13  0.861526  0.709835  0.136232  0.985813  0.488873\n",
            "14  0.351203  0.066233  0.493564  0.972598  0.322743\n",
            "15  1.000000  1.000000  1.000000  1.000000  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "normalized_scores = {model: (sim + 1) / 2 for model, sim in cosine_similarities.items()}\n",
        "\n",
        "normalized_scores_array = np.array(list(normalized_scores.values()))\n"
      ],
      "metadata": {
        "id": "TvHvq6YfMrYX"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ideal_solution = np.max(normalized_scores_array, axis=0)\n",
        "anti_ideal_solution = np.min(normalized_scores_array, axis=0)\n"
      ],
      "metadata": {
        "id": "gE08273OPm5-"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "distance_to_ideal = np.linalg.norm(normalized_scores_array - ideal_solution, axis=1)\n",
        "distance_to_anti_ideal = np.linalg.norm(normalized_scores_array - anti_ideal_solution, axis=1)\n"
      ],
      "metadata": {
        "id": "E2LWPVQHPrZi"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "closeness_coefficient = np.divide(distance_to_anti_ideal, (distance_to_ideal + distance_to_anti_ideal),\n",
        "                                  out=np.zeros_like(distance_to_anti_ideal), where=(distance_to_ideal + distance_to_anti_ideal) != 0)\n"
      ],
      "metadata": {
        "id": "eHWcW7eIPtpI"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ranked_models = sorted(zip(model_sentences.keys(), closeness_coefficient[:, 0]), key=lambda x: x[1], reverse=True)\n",
        "df = pd.DataFrame(ranked_models, columns=[\"Model\", \"Closeness Coefficient\"])\n",
        "\n",
        "df[\"Rank\"] = df[\"Closeness Coefficient\"].rank(ascending=False).astype(int)\n",
        "\n",
        "print( df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_ioUJjoPvVa",
        "outputId": "c4b0e832-0aca-4b91-db3a-dab88bb72c73"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model  Closeness Coefficient  Rank\n",
            "RoBERTa               1.000000     1\n",
            "   BERT               0.402476     2\n",
            "  XLNet               0.333772     3\n",
            "    USE               0.290922     4\n",
            " Tf-Idf               0.279481     5\n"
          ]
        }
      ]
    }
  ]
}